{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Parkinson's Disease\n",
    "This notebook uses the the dataset from [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification) to practice classification problem through popular classification models. For each model, the algorithm, and a related topic if any, will be discussed before application. Afterwards, there will be a review on modeling performance. \n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "### I. Data Preparation\n",
    "- Load and Process Data\n",
    "- Problem Definition\n",
    "\n",
    "### II. Modeling\n",
    "\n",
    "Each modeling section consists of **an brief intro, the algorithm, discussion on related topics, and application** on the dataset.\n",
    "\n",
    "1. kNN\n",
    "    - Non-parametric Models\n",
    "    - Algorithm\n",
    "2. Naive Bayes  \n",
    "    - Bayes Classifier\n",
    "    - Algorithm\n",
    "    - Generative Model vs. Discriminative Model\n",
    "3. Logistic Regression\n",
    "    - Sigmoid Function\n",
    "    - Maximum Likelihood Estimation\n",
    "    - Algorithm\n",
    "    - (Also see Appendix A & B for related topics)\n",
    "4. Support Vector Machine\n",
    "    - Convex Sets and Convex Hulls\n",
    "    - Algorithm\n",
    "    - Soft-Margin SVM\n",
    "5. Kernel SVM\n",
    "    - Kernel\n",
    "    - Mercer's theorem\n",
    "    - RBF\n",
    "    - Algorithm\n",
    "6. Decision Tree\n",
    "\n",
    "### III. Model Improvement\n",
    "- PCA\n",
    "- Pipeline\n",
    "\n",
    "### IV. Model Selection\n",
    "- ROC Curve\n",
    "- Classification Report\n",
    "\n",
    "### V. Appendix\n",
    "- Appendix A: Concepts for Logistic Regression\n",
    "    - A1. Binary Classification\n",
    "    - A2. Log Odds\n",
    "    - A3. Linear Discriminant Analysis\n",
    "\n",
    "- Appendix B: Linear Classifiers\n",
    "    - B1. Definition\n",
    "    - B2. Linear Separability\n",
    "    - B3. Methods\n",
    "    \n",
    "### References for Model Introduction and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preparation\n",
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../project-parkinsons-disease-classification/data/pd_speech_features.csv', \n",
    "                 header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 755)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 755 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0   0       1  0.85247  0.71826  0.57227        240               239   \n",
       "1   0       1  0.76686  0.69481  0.53966        234               233   \n",
       "2   0       1  0.85083  0.67604  0.58982        232               231   \n",
       "3   1       0  0.41121  0.79672  0.59257        178               177   \n",
       "4   1       0  0.32790  0.79782  0.53028        236               235   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  ...    \\\n",
       "0          0.008064            0.000087       0.00218  ...     \n",
       "1          0.008258            0.000073       0.00195  ...     \n",
       "2          0.008340            0.000060       0.00176  ...     \n",
       "3          0.010858            0.000183       0.00419  ...     \n",
       "4          0.008162            0.002669       0.00535  ...     \n",
       "\n",
       "   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                     1.5620                     2.6445   \n",
       "1                     1.5589                     3.6107   \n",
       "2                     1.5643                     2.3308   \n",
       "3                     3.7805                     3.5664   \n",
       "4                     6.1727                     5.8416   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                     3.8686                     4.2105   \n",
       "1                    23.5155                    14.1962   \n",
       "2                     9.4959                    10.7458   \n",
       "3                     5.2558                    14.0403   \n",
       "4                     6.0805                     5.7621   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                     5.1221                     4.4625   \n",
       "1                    11.0261                     9.5082   \n",
       "2                    11.0177                     4.8066   \n",
       "3                     4.2235                     4.6857   \n",
       "4                     7.7817                    11.6891   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                     2.6202                     3.0004   \n",
       "1                     6.5245                     6.3431   \n",
       "2                     2.9199                     3.1495   \n",
       "3                     4.8460                     6.2650   \n",
       "4                     8.2103                     5.0559   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  class  \n",
       "0                    18.9405      1  \n",
       "1                    45.1780      1  \n",
       "2                     4.7666      1  \n",
       "3                     4.0603      1  \n",
       "4                     6.1164      1  \n",
       "\n",
       "[5 rows x 755 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column, class, is the target variable that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of attribute datatypes:\n",
      "float64    749\n",
      "int64        6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Summary of attribute datatypes:\\n', df.dtypes.value_counts(), sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 3 records for each patient, and all the columns, except for gender, seem to be numeric. Therefore, gender will be transformed into categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender = df.gender.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id column and get X and y datasets\n",
    "X = df.drop('id', axis = 1).iloc[:,:-1]\n",
    "y = df.drop('id', axis = 1).iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5466</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5530</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5399</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9761</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8832</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0      1  0.85247  0.71826  0.57227        240               239   \n",
       "1      1  0.76686  0.69481  0.53966        234               233   \n",
       "2      1  0.85083  0.67604  0.58982        232               231   \n",
       "3      0  0.41121  0.79672  0.59257        178               177   \n",
       "4      0  0.32790  0.79782  0.53028        236               235   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  \\\n",
       "0          0.008064            0.000087       0.00218      0.000018   \n",
       "1          0.008258            0.000073       0.00195      0.000016   \n",
       "2          0.008340            0.000060       0.00176      0.000015   \n",
       "3          0.010858            0.000183       0.00419      0.000046   \n",
       "4          0.008162            0.002669       0.00535      0.000044   \n",
       "\n",
       "             ...              tqwt_kurtosisValue_dec_27  \\\n",
       "0            ...                                 1.5466   \n",
       "1            ...                                 1.5530   \n",
       "2            ...                                 1.5399   \n",
       "3            ...                                 6.9761   \n",
       "4            ...                                 7.8832   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                     1.5620                     2.6445   \n",
       "1                     1.5589                     3.6107   \n",
       "2                     1.5643                     2.3308   \n",
       "3                     3.7805                     3.5664   \n",
       "4                     6.1727                     5.8416   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                     3.8686                     4.2105   \n",
       "1                    23.5155                    14.1962   \n",
       "2                     9.4959                    10.7458   \n",
       "3                     5.2558                    14.0403   \n",
       "4                     6.0805                     5.7621   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                     5.1221                     4.4625   \n",
       "1                    11.0261                     9.5082   \n",
       "2                    11.0177                     4.8066   \n",
       "3                     4.2235                     4.6857   \n",
       "4                     7.7817                    11.6891   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                     2.6202                     3.0004   \n",
       "1                     6.5245                     6.3431   \n",
       "2                     2.9199                     3.1495   \n",
       "3                     4.8460                     6.2650   \n",
       "4                     8.2103                     5.0559   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  \n",
       "0                    18.9405  \n",
       "1                    45.1780  \n",
       "2                     4.7666  \n",
       "3                     4.0603  \n",
       "4                     6.1164  \n",
       "\n",
       "[5 rows x 753 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    564\n",
       "0    192\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_origin, X_test_origin, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0</td>\n",
       "      <td>0.84056</td>\n",
       "      <td>0.65446</td>\n",
       "      <td>0.47962</td>\n",
       "      <td>347</td>\n",
       "      <td>346</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4510</td>\n",
       "      <td>122.7173</td>\n",
       "      <td>92.5575</td>\n",
       "      <td>68.1938</td>\n",
       "      <td>33.0630</td>\n",
       "      <td>9.9553</td>\n",
       "      <td>4.4652</td>\n",
       "      <td>4.6952</td>\n",
       "      <td>7.6986</td>\n",
       "      <td>6.2858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0.81038</td>\n",
       "      <td>0.57581</td>\n",
       "      <td>0.55182</td>\n",
       "      <td>378</td>\n",
       "      <td>377</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>14.6142</td>\n",
       "      <td>10.8624</td>\n",
       "      <td>8.2429</td>\n",
       "      <td>7.7884</td>\n",
       "      <td>6.0109</td>\n",
       "      <td>3.9348</td>\n",
       "      <td>3.1266</td>\n",
       "      <td>11.7028</td>\n",
       "      <td>14.3225</td>\n",
       "      <td>44.4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0</td>\n",
       "      <td>0.85203</td>\n",
       "      <td>0.67648</td>\n",
       "      <td>0.40249</td>\n",
       "      <td>420</td>\n",
       "      <td>419</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1270</td>\n",
       "      <td>8.2350</td>\n",
       "      <td>7.7018</td>\n",
       "      <td>6.7051</td>\n",
       "      <td>5.4480</td>\n",
       "      <td>5.9582</td>\n",
       "      <td>6.5177</td>\n",
       "      <td>5.8359</td>\n",
       "      <td>6.8411</td>\n",
       "      <td>10.7085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>1</td>\n",
       "      <td>0.82006</td>\n",
       "      <td>0.66421</td>\n",
       "      <td>0.37479</td>\n",
       "      <td>291</td>\n",
       "      <td>290</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5892</td>\n",
       "      <td>39.3346</td>\n",
       "      <td>53.8215</td>\n",
       "      <td>17.3874</td>\n",
       "      <td>11.8498</td>\n",
       "      <td>11.4329</td>\n",
       "      <td>6.3404</td>\n",
       "      <td>4.1468</td>\n",
       "      <td>3.9736</td>\n",
       "      <td>3.5569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80080</td>\n",
       "      <td>0.67812</td>\n",
       "      <td>0.59670</td>\n",
       "      <td>230</td>\n",
       "      <td>229</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6766</td>\n",
       "      <td>1.7082</td>\n",
       "      <td>2.8944</td>\n",
       "      <td>5.8136</td>\n",
       "      <td>4.8728</td>\n",
       "      <td>4.7480</td>\n",
       "      <td>7.5865</td>\n",
       "      <td>24.5939</td>\n",
       "      <td>28.8671</td>\n",
       "      <td>62.1721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "649      0  0.84056  0.65446  0.47962        347               346   \n",
       "53       0  0.81038  0.57581  0.55182        378               377   \n",
       "707      0  0.85203  0.67648  0.40249        420               419   \n",
       "740      1  0.82006  0.66421  0.37479        291               290   \n",
       "121      1  0.80080  0.67812  0.59670        230               229   \n",
       "\n",
       "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  \\\n",
       "649          0.005573            0.000075       0.00147      0.000008   \n",
       "53           0.005109            0.000056       0.00113      0.000006   \n",
       "707          0.004594            0.000038       0.00086      0.000004   \n",
       "740          0.006629            0.000084       0.00154      0.000010   \n",
       "121          0.008413            0.000076       0.00269      0.000023   \n",
       "\n",
       "               ...              tqwt_kurtosisValue_dec_27  \\\n",
       "649            ...                                 7.4510   \n",
       "53             ...                                14.6142   \n",
       "707            ...                                 9.1270   \n",
       "740            ...                                 1.5892   \n",
       "121            ...                                 1.6766   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "649                   122.7173                    92.5575   \n",
       "53                     10.8624                     8.2429   \n",
       "707                     8.2350                     7.7018   \n",
       "740                    39.3346                    53.8215   \n",
       "121                     1.7082                     2.8944   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "649                    68.1938                    33.0630   \n",
       "53                      7.7884                     6.0109   \n",
       "707                     6.7051                     5.4480   \n",
       "740                    17.3874                    11.8498   \n",
       "121                     5.8136                     4.8728   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "649                     9.9553                     4.4652   \n",
       "53                      3.9348                     3.1266   \n",
       "707                     5.9582                     6.5177   \n",
       "740                    11.4329                     6.3404   \n",
       "121                     4.7480                     7.5865   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "649                     4.6952                     7.6986   \n",
       "53                     11.7028                    14.3225   \n",
       "707                     5.8359                     6.8411   \n",
       "740                     4.1468                     3.9736   \n",
       "121                    24.5939                    28.8671   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_36  \n",
       "649                     6.2858  \n",
       "53                     44.4396  \n",
       "707                    10.7085  \n",
       "740                     3.5569  \n",
       "121                    62.1721  \n",
       "\n",
       "[5 rows x 753 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_origin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.561762</td>\n",
       "      <td>-0.653931</td>\n",
       "      <td>-0.060365</td>\n",
       "      <td>0.258945</td>\n",
       "      <td>0.261741</td>\n",
       "      <td>-0.444989</td>\n",
       "      <td>-0.416778</td>\n",
       "      <td>-0.328500</td>\n",
       "      <td>-0.381641</td>\n",
       "      <td>-0.463235</td>\n",
       "      <td>...</td>\n",
       "      <td>2.316302</td>\n",
       "      <td>2.215421</td>\n",
       "      <td>1.952058</td>\n",
       "      <td>0.936115</td>\n",
       "      <td>-0.135010</td>\n",
       "      <td>-0.488561</td>\n",
       "      <td>-0.650781</td>\n",
       "      <td>-0.503318</td>\n",
       "      <td>-0.759712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381634</td>\n",
       "      <td>-1.792895</td>\n",
       "      <td>0.459661</td>\n",
       "      <td>0.585070</td>\n",
       "      <td>0.587141</td>\n",
       "      <td>-0.706114</td>\n",
       "      <td>-0.443094</td>\n",
       "      <td>-0.460398</td>\n",
       "      <td>-0.491451</td>\n",
       "      <td>-0.452747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355319</td>\n",
       "      <td>-0.441892</td>\n",
       "      <td>-0.417754</td>\n",
       "      <td>-0.389291</td>\n",
       "      <td>-0.467199</td>\n",
       "      <td>-0.568877</td>\n",
       "      <td>-0.210502</td>\n",
       "      <td>-0.050324</td>\n",
       "      <td>0.341487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630221</td>\n",
       "      <td>-0.335050</td>\n",
       "      <td>-0.615900</td>\n",
       "      <td>1.026916</td>\n",
       "      <td>1.028005</td>\n",
       "      <td>-0.995878</td>\n",
       "      <td>-0.468850</td>\n",
       "      <td>-0.565140</td>\n",
       "      <td>-0.574489</td>\n",
       "      <td>-0.515674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418073</td>\n",
       "      <td>-0.458945</td>\n",
       "      <td>-0.460254</td>\n",
       "      <td>-0.416870</td>\n",
       "      <td>-0.355555</td>\n",
       "      <td>-0.365409</td>\n",
       "      <td>-0.579112</td>\n",
       "      <td>-0.561961</td>\n",
       "      <td>-0.632064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.439409</td>\n",
       "      <td>-0.512737</td>\n",
       "      <td>-0.815411</td>\n",
       "      <td>-0.330183</td>\n",
       "      <td>-0.326078</td>\n",
       "      <td>0.148934</td>\n",
       "      <td>-0.404459</td>\n",
       "      <td>-0.301344</td>\n",
       "      <td>-0.290889</td>\n",
       "      <td>-0.358357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324731</td>\n",
       "      <td>0.994593</td>\n",
       "      <td>-0.041168</td>\n",
       "      <td>-0.103217</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>-0.376048</td>\n",
       "      <td>-0.685236</td>\n",
       "      <td>-0.758063</td>\n",
       "      <td>-0.838474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.324456</td>\n",
       "      <td>-0.311301</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>-0.971912</td>\n",
       "      <td>-0.966380</td>\n",
       "      <td>1.152510</td>\n",
       "      <td>-0.414958</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.271774</td>\n",
       "      <td>0.113595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573964</td>\n",
       "      <td>-0.610458</td>\n",
       "      <td>-0.495229</td>\n",
       "      <td>-0.445052</td>\n",
       "      <td>-0.422329</td>\n",
       "      <td>-0.301281</td>\n",
       "      <td>0.599431</td>\n",
       "      <td>0.944349</td>\n",
       "      <td>0.853285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PPE       DFA      RPDE  numPulses  numPeriodsPulses  \\\n",
       "0  0.561762 -0.653931 -0.060365   0.258945          0.261741   \n",
       "1  0.381634 -1.792895  0.459661   0.585070          0.587141   \n",
       "2  0.630221 -0.335050 -0.615900   1.026916          1.028005   \n",
       "3  0.439409 -0.512737 -0.815411  -0.330183         -0.326078   \n",
       "4  0.324456 -0.311301  0.782912  -0.971912         -0.966380   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  \\\n",
       "0         -0.444989           -0.416778     -0.328500     -0.381641   \n",
       "1         -0.706114           -0.443094     -0.460398     -0.491451   \n",
       "2         -0.995878           -0.468850     -0.565140     -0.574489   \n",
       "3          0.148934           -0.404459     -0.301344     -0.290889   \n",
       "4          1.152510           -0.414958      0.144782      0.271774   \n",
       "\n",
       "   rapJitter   ...    tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0  -0.463235   ...                     2.316302                   2.215421   \n",
       "1  -0.452747   ...                    -0.355319                  -0.441892   \n",
       "2  -0.515674   ...                    -0.418073                  -0.458945   \n",
       "3  -0.358357   ...                     0.324731                   0.994593   \n",
       "4   0.113595   ...                    -0.573964                  -0.610458   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                   1.952058                   0.936115   \n",
       "1                  -0.417754                  -0.389291   \n",
       "2                  -0.460254                  -0.416870   \n",
       "3                  -0.041168                  -0.103217   \n",
       "4                  -0.495229                  -0.445052   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                  -0.135010                  -0.488561   \n",
       "1                  -0.467199                  -0.568877   \n",
       "2                  -0.355555                  -0.365409   \n",
       "3                  -0.053482                  -0.376048   \n",
       "4                  -0.422329                  -0.301281   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                  -0.650781                  -0.503318   \n",
       "1                  -0.210502                  -0.050324   \n",
       "2                  -0.579112                  -0.561961   \n",
       "3                  -0.685236                  -0.758063   \n",
       "4                   0.599431                   0.944349   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  gender  \n",
       "0                  -0.759712       0  \n",
       "1                   0.341487       0  \n",
       "2                  -0.632064       0  \n",
       "3                  -0.838474       1  \n",
       "4                   0.853285       1  \n",
       "\n",
       "[5 rows x 753 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_origin[X_train_origin.columns[1:]])\n",
    "X_train = pd.DataFrame(scaler.transform(X_train_origin[X_train_origin.columns[1:]]))\n",
    "X_train.columns = X_train_origin.columns[1:]\n",
    "X_train['gender'] = X_train_origin['gender'].values\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.818168</td>\n",
       "      <td>1.382591</td>\n",
       "      <td>-0.879802</td>\n",
       "      <td>0.784953</td>\n",
       "      <td>0.786580</td>\n",
       "      <td>-0.845439</td>\n",
       "      <td>-0.446453</td>\n",
       "      <td>-0.344017</td>\n",
       "      <td>-0.437453</td>\n",
       "      <td>-0.337381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576704</td>\n",
       "      <td>-0.629740</td>\n",
       "      <td>-0.618386</td>\n",
       "      <td>-0.564687</td>\n",
       "      <td>-0.524389</td>\n",
       "      <td>-0.264333</td>\n",
       "      <td>0.134366</td>\n",
       "      <td>0.120549</td>\n",
       "      <td>1.026847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.962247</td>\n",
       "      <td>-0.892006</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>1.752806</td>\n",
       "      <td>1.752282</td>\n",
       "      <td>-1.363024</td>\n",
       "      <td>-0.415098</td>\n",
       "      <td>-0.642728</td>\n",
       "      <td>-0.634839</td>\n",
       "      <td>-0.557625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795788</td>\n",
       "      <td>-0.415657</td>\n",
       "      <td>-0.595251</td>\n",
       "      <td>-0.515540</td>\n",
       "      <td>-0.517095</td>\n",
       "      <td>-0.603414</td>\n",
       "      <td>-0.774051</td>\n",
       "      <td>-0.845032</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539500</td>\n",
       "      <td>-0.555747</td>\n",
       "      <td>-0.303524</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>-0.732531</td>\n",
       "      <td>-0.406979</td>\n",
       "      <td>-0.464277</td>\n",
       "      <td>-0.496442</td>\n",
       "      <td>-0.431771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463072</td>\n",
       "      <td>-0.584132</td>\n",
       "      <td>-0.594474</td>\n",
       "      <td>-0.467893</td>\n",
       "      <td>-0.468380</td>\n",
       "      <td>-0.598242</td>\n",
       "      <td>-0.753933</td>\n",
       "      <td>-0.858504</td>\n",
       "      <td>-0.856640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.611898</td>\n",
       "      <td>-2.010551</td>\n",
       "      <td>0.474138</td>\n",
       "      <td>1.731766</td>\n",
       "      <td>1.731289</td>\n",
       "      <td>-1.351677</td>\n",
       "      <td>-0.477949</td>\n",
       "      <td>-0.627210</td>\n",
       "      <td>-0.628486</td>\n",
       "      <td>-0.536649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.563917</td>\n",
       "      <td>1.377564</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.144995</td>\n",
       "      <td>0.283430</td>\n",
       "      <td>0.591353</td>\n",
       "      <td>0.724247</td>\n",
       "      <td>0.098980</td>\n",
       "      <td>-0.410463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.701007</td>\n",
       "      <td>-1.642433</td>\n",
       "      <td>0.402256</td>\n",
       "      <td>1.752806</td>\n",
       "      <td>1.752282</td>\n",
       "      <td>-1.361762</td>\n",
       "      <td>-0.397880</td>\n",
       "      <td>-0.634969</td>\n",
       "      <td>-0.631209</td>\n",
       "      <td>-0.568113</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074254</td>\n",
       "      <td>1.418753</td>\n",
       "      <td>1.928198</td>\n",
       "      <td>2.571426</td>\n",
       "      <td>2.983018</td>\n",
       "      <td>2.906571</td>\n",
       "      <td>2.495133</td>\n",
       "      <td>2.150136</td>\n",
       "      <td>1.743479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PPE       DFA      RPDE  numPulses  numPeriodsPulses  \\\n",
       "0  0.818168  1.382591 -0.879802   0.784953          0.786580   \n",
       "1  0.962247 -0.892006  0.009356   1.752806          1.752282   \n",
       "2  0.539500 -0.555747 -0.303524   0.616630          0.618632   \n",
       "3  0.611898 -2.010551  0.474138   1.731766          1.731289   \n",
       "4  0.701007 -1.642433  0.402256   1.752806          1.752282   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  \\\n",
       "0         -0.845439           -0.446453     -0.344017     -0.437453   \n",
       "1         -1.363024           -0.415098     -0.642728     -0.634839   \n",
       "2         -0.732531           -0.406979     -0.464277     -0.496442   \n",
       "3         -1.351677           -0.477949     -0.627210     -0.628486   \n",
       "4         -1.361762           -0.397880     -0.634969     -0.631209   \n",
       "\n",
       "   rapJitter   ...    tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0  -0.337381   ...                    -0.576704                  -0.629740   \n",
       "1  -0.557625   ...                     0.795788                  -0.415657   \n",
       "2  -0.431771   ...                    -0.463072                  -0.584132   \n",
       "3  -0.536649   ...                     1.563917                   1.377564   \n",
       "4  -0.568113   ...                     1.074254                   1.418753   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                  -0.618386                  -0.564687   \n",
       "1                  -0.595251                  -0.515540   \n",
       "2                  -0.594474                  -0.467893   \n",
       "3                   0.650900                   0.144995   \n",
       "4                   1.928198                   2.571426   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                  -0.524389                  -0.264333   \n",
       "1                  -0.517095                  -0.603414   \n",
       "2                  -0.468380                  -0.598242   \n",
       "3                   0.283430                   0.591353   \n",
       "4                   2.983018                   2.906571   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                   0.134366                   0.120549   \n",
       "1                  -0.774051                  -0.845032   \n",
       "2                  -0.753933                  -0.858504   \n",
       "3                   0.724247                   0.098980   \n",
       "4                   2.495133                   2.150136   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  gender  \n",
       "0                   1.026847       0  \n",
       "1                  -0.854498       0  \n",
       "2                  -0.856640       0  \n",
       "3                  -0.410463       0  \n",
       "4                   1.743479       0  \n",
       "\n",
       "[5 rows x 753 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.DataFrame(scaler.transform(X_test_origin[X_test_origin.columns[1:]]))\n",
    "X_test.columns = X_test_origin.columns[1:]\n",
    "X_test['gender'] = X_test_origin['gender'].values\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    451\n",
       "0    153\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "**Input:** $X \\in \\mathbb{R}^{d}, d = 243$\n",
    "\n",
    "**Output:** $Y = \\{0, 1\\}$\n",
    "\n",
    "**Classifier:** Calssification uses a function $f$ (called a classifier) to map input $x$ to class $y$. \n",
    "\n",
    "$y = f(x) : f$ takes in $x \\in X$ and declares its class to be $y \\in Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. k-Nearest Neighbors Classifier (kNN)\n",
    "KNNs classify the unseen instance based on the K points in the training set which are nearest to it. It is a **non-parametric method**. \n",
    "\n",
    "#### Non-parametric Models\n",
    "Non-parametric models differ from parametric models in that the model structure is not specified a priori but is instead determined from data. The term non-parametric is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Nonparametric_statistics#Non-parametric_models\n",
    "\n",
    "#### Algorithm\n",
    "Given data $(x_1, y_1),...,(x_n, y_n)$, construct the $k$-NN classifier as follows:\n",
    "For a new input $s$,\n",
    "1. Return the $k$ points closest to $x$, indexed as $x_{i_1},...x_{i_k}$.\n",
    "2. Return the majority-vote of $y_{i_1}, y_{i_2},..., y_{i_k}$.\n",
    "The default distance for data in $\\mathbb{R}^d$ is the Euclidean one:\n",
    "$$\\|u-v\\|_2 = \\big(\\sum_{i=1}^{d}(u_i-v_i)^2\\big)^\\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Neighbors: {'n_neighbors': 3}\n",
      "Accuracy on Training Set: 0.864238410596\n",
      "Accuracy on Test Set: 0.855263157895\n",
      "AUC: 0.905831631495\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(3, 13)}\n",
    "gs_kNN = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "gs_kNN.fit(X_train, y_train)\n",
    "print(\"Best Number of Neighbors:\", gs_kNN.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_kNN.best_score_)\n",
    "\n",
    "y_pred_prob = gs_kNN.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_kNN.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe how the accuracy change as k grows\n",
    "Note: The accuracy is different from the results from grid search because the whole training set is used to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training and test errors by k\n",
    "training_error = list()\n",
    "test_error = list()\n",
    "for k in np.arange(3, 13):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    training_error.append(knn.score(X_train, y_train))\n",
    "    test_error.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29fee162f98>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_error, label = 'Training set accuracy')\n",
    "plt.plot(test_error, label = 'Test set accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, 10), np.arange(3, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes \n",
    "\n",
    "With Naive Bayes classifier we predict the class of a new $x$ to be the most probable lable given model and training data $(x_1, y_1), ..., (x_n, y_n)$.\n",
    "\n",
    "#### Bayes Classifier\n",
    "Before talking about the algorithm of Naive Bayes, we need to know **Bayes Classifer**:\n",
    "\n",
    "$$f(x) = \\operatorname*{arg\\,max}_{y \\in Y} P(Y = y| X = x)$$\n",
    "\n",
    "For a particular input $x$, predict the label to be the most probable label conditioned on $x$ according to the true underlying distribution given to us from nature.\n",
    "\n",
    "From Bayes rule we equivalently have\n",
    "$$ f(x) \\approx \\operatorname*{arg\\,max}_{y \\in Y} P(Y = y) \\times P(Y = y| X = x) $$\n",
    "- $P(Y = y)$ is called the $\\textit{class prior}$.\n",
    "- $P(X = x|Y = y)$ is called the $\\textit{class conditional distribution}$ of X.\n",
    "- In practice we don't know either of these, so we approximate them.\n",
    "\n",
    "Aside: If $X$ is a continuous-valued random variable, replace $P(X = x|Y = y)$ with class conditional density $p(x|Y=y)$.\n",
    "\n",
    "Problem: We can't construct the Bayes classifier without knowing $P(Y = y|X = x)$, or equv., $P(X = x|Y = y)$ and $P(Y = y)$. All we have are labeled examples drowm from the distribution.\n",
    "\n",
    "#### Naive Bayes Algorithm\n",
    "We have to $\\textit{define } p(X = x|Y = y)$.\n",
    "\n",
    "Naive Bayes is a Bayes classifier that makes the assumption\n",
    "$$p(X = x|Y = y) = \\prod_{j=1}^{d}p_j(x(j)|Y = y),$$\n",
    "i.e., it treats the dimension of $X$ as $\\textit{conditionally independent}$ given $y$. \n",
    "\n",
    "Note: Each mimension might not be independent with each other, but they are conditionally independent given y.\n",
    "\n",
    "#### Discriminative Model vs. Generative Model\n",
    "\n",
    "Bayes Classifer and Naive Bayes are ***generative models***. Unlike distriminative models, generative models consider the joint probability distribution on $X \\times y, P(X, y)$, make the prediction on the probability of each lable $y$ given $X$, $p(y|X = x)$, and then pick the most likely lable $y$.\n",
    "\n",
    "##### Discriminative Algorithms\n",
    "- Idea: model $p(y|x)$, conditional distribution of $y$ given $x$.\n",
    "- In Discriminative Algorithms: find a decision boundary that separates positive from negative example.\n",
    "- To predict a new example, chec on which side of the decision boundary it falls.\n",
    "- Model $p(y|x)$ directly\n",
    "\n",
    "##### Generative Algorithms\n",
    "- Idea: Build a model for what positive examples look like and build a different model for what negative example look like.\n",
    "- To predict a new example, match it with each of the models and see which match is best.\n",
    "- Model $p(x|y)$ and $p(y)$!\n",
    "- Use Bayes rule to obtain $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$\n",
    "\n",
    "The definition of generative and distriminative models:\n",
    "- a generative model is a model of the conditional probability of the observable $X$, given a target $y$, symbolically, $P(X|Y=y)$\n",
    "- a discriminative model is a model of the conditional probability of the target $Y$, given an observation $x$, symbolically, $P(Y|X=x)$\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Generative_model#Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.809602649007\n",
      "Accuracy on test set: 0.736842105263\n",
      "AUC: 0.739505332426\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_prob = gnb.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Accuracy on training set:\", gnb.score(X_train, y_train))\n",
    "print(\"Accuracy on test set:\", gnb.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression\n",
    "Let $(x_1, y_1),...,(x_n, y_n)$ be a set of binary labeled data with $y \\in {-1, +1}$. $\\textit{Logistic regression}$ models each $y_i$ as independently generated, with\n",
    "\n",
    "$$P(y_i = +1|x_i, w) = \\sigma(x_i^Tw), \\ \\ \\sigma(x_i; w) = \\frac{e^{x_iTw}}{1 + e^{x_iTw}}.$$\n",
    "\n",
    "#### Sigmoid Function\n",
    "From **Linear Discriminative Analysis**, we can directly plug in the **hyperplane representation** for the **log odds**. ***(See Appendix A: Concepts for Logistic Regression & Appendix B: Linear Classifiers)***\n",
    "\n",
    "$$\\ln\\frac{p(y = +1|x)}{p(y = -1|x)} = x^Tw + w_0$$\n",
    "\n",
    "Note: No restrictions on $w$ and $w_0$ compared to LDA.\n",
    "\n",
    "Setting $p(y = -1|x) = 1 - p(y = -1|x)$, solve for $p(y = +1|x)$ to find \n",
    "$$ p(y = +1|x) = \\frac{exp^{x^Tw + w_0}}{1 + exp^{x^Tw + w_0}} = \\sigma(x^tw + w_0). $$\n",
    "\n",
    "- This is called the sigmoid function\n",
    "- We have chosen $x^Tw + w_0$ as the $\\textit{link function}$ for log odds.\n",
    "- If $x^Tw > 0$, then $\\sigma(x^Tw) > 1/2$ and predict $y = +1$, and vice versa.\n",
    "- We now get **a confidence in our prediction** via the probability of $\\sigma (x^Tw)$.\n",
    "\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "Define $\\sigma_i(w) = \\sigma(x_i^Tw)$. The joint likihood of $y_1,...y_n$ is\n",
    "$$p(y_1,...,y_n|x_1,...,x_n, w) = \\prod_{i=1}^{n}p(y_i|x_i, w)\n",
    "= \\prod_{i=1}^{n}\\sigma_i(w)^{\\mathbb{I}(y_i=+1)}(1-\\sigma_i(w))^{\\mathbb{I}(y_i=-1)} \n",
    "= \\prod_{i=1}^{n}\\sigma_i(y_i \\cdot w)$$\n",
    "\n",
    "Note: here y = {+1, -1}\n",
    "\n",
    "we want to maximize this over $w$.\n",
    "\n",
    "The maximum likelihood solution for $w$ can be written \n",
    "$$ w_{ML} = \\operatorname*{arg\\,max}_{w} \\sum_{i=1}^{n}\\ln\\sigma_i(y_i\\cdot w)\n",
    "= \\operatorname*{arg\\,max}_{w} L $$\n",
    "\n",
    "We can't directly set $\\nabla_w L = 0$, so we need an iterative algorithm. At step $t$, we can update\n",
    "$$ w^{(t+1)}=w^{(t)}+\\eta\\nabla_wL \\ \\ \\nabla_wL = \\sum_{i=1}{n}(1-\\sigma_i(y_i\\cdot w))y_ix_i $$\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "**Input**: Training data $(x_1, y_1),...,(x_n, y_n)$ and step size $\\eta > 0$\n",
    "1. **Set** $w^{(1)} = \\overrightarrow{0}$\n",
    "2. **For step** $t = 1,2,...$ **do**\n",
    "    - Update $w^{(t+1)} = w^{(t)} + \\eta\\sum_{i=1}^{n}(1-\\sigma_i(y_i\\cdot w))y_ix_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build logistic model with L2 regularization\n",
    "param_grid = {'penalty': ['l1', 'l2'],\n",
    "              'C': [0.1, 1, 10, 100], \n",
    "              'tol': [1e-4, 1e-5], \n",
    "              'max_iter': [100, 500]}\n",
    "gs_lr = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_lr.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_lr.best_score_)\n",
    "\n",
    "y_pred_prob = gs_lr.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_lr.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.79862153e-05   9.99982014e-01]\n",
      " [  9.96741352e-01   3.25864761e-03]\n",
      " [  9.99969169e-01   3.08313593e-05]\n",
      " [  1.18302932e-02   9.88169707e-01]\n",
      " [  5.25225892e-01   4.74774108e-01]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-ecd1a792c6fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_prob = gs_lr.predict_proba(X_test)\n",
    "print(y_pred_prob[:5])\n",
    "print(y_pred[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the threshold does not improve prediction\n",
    "for t in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    y_pred_rev = [1 if i[1] > t else 0 for i in y_pred_prob]\n",
    "    print('Threshold: {}, Accuracy: {}, AUC: {}'.format(t, accuracy_score(y_pred_rev, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When threshold is changed to 0.55, the accuracy and AUC is higher than the original prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Support Vector Machine\n",
    "\n",
    "With two linearly separable classes, choose a hyperplane such that its distance to the **closest point in each class** is maximized to achieve good generalization (low prediction error).\n",
    "\n",
    "#### Convex Sets and Convex Hulls\n",
    "Where a seperating hyperplane may be placed depends on the \"outer\" points on the sets. Points in the center do not matter. In geometric terms, we can represent each class by the smallest convex set which contains all point in the class. This is called a $\\textit{convex hull}$.\n",
    "\n",
    "A convex hull is defined by all possible weighted averages of points in a set. That is, let $x_1,...x_n$ be the data coordinates. Every point $x_0$ in the convex hull can be reached by setting\n",
    "$$ x_0 = \\sum_{i=1}^{n}\\alpha_1x_1, \\ \\ \\alpha_i \\geq0, \\ \\ \\sum_{i=1}^{n}\\alpha_1 = 1,$$\n",
    "for some $(\\alpha_1,...\\alpha_n)$. No point outside the convex hull can be reached this way.\n",
    "\n",
    "#### Algorithm\n",
    "For $n$ seperate points $(x_1,y_1),...,(x_n,y_n)$ with $y_i \\in {\\pm1}$, solve:\n",
    "$$ \\min_{w, w_0} \\frac{1}{2}\\|w\\|^2 $$\n",
    "subject to\n",
    "$$ y_i(x_i^Tw + w_0) \\geq 1 \\ \\ for\\ i = 1,...,n$$\n",
    "\n",
    "- If there exists a hyperplan $H$ that separates the classes, we can scale $w$ so that $y_i(x_i^Tw+w_0)>1$ for all $i$.\n",
    "- This formula only has a solution when the classes are linearly separable.\n",
    "\n",
    "Solving above foluma would require $\\textit{Lagrange multipliers}$. After derived with $\\textit{Lagrange multipliers}$, the formula will eventually turn into:\n",
    "$$\\min_{\\alpha_1,...\\alpha_n}\\bigg|\\bigg(\\sum_{i\\in S_1}\\frac{\\alpha_i}{C}x_i\\bigg) - \n",
    "\\bigg(\\sum_{j\\in S_0}\\frac{\\alpha_j}{C}x_j\\bigg)\\bigg|^2,$$\n",
    "where \n",
    "- $S_i$ and $S_0$ are the sets of $x$ in class $+1$ and $-1$\n",
    "- $C:=\\sum_{i\\in S_1}\\alpha_u = \\sum_{j\\in S_0}\\alpha_j,\\ \\ \\alpha_i\\geq0$\n",
    "\n",
    "Therefore, the algorithm is to find the closest points in the convex hulls constructed from the data in class $+1$ and $-1$.\n",
    "\n",
    "#### Soft-Margin SVM\n",
    "\n",
    "If the data isn't linearly separable, permit training data be on wrong side of hyperplane at a cost by replacing the training rule $y_i(x_i^Tw + w_0) \\geq 1$  with \n",
    "$$y_i(x_i^Tw+w_0)\\geq 1 - \\xi_i, \\ \\ with \\ \\ \\xi_i \\geq0.$$\n",
    "\n",
    "The $\\xi_i$ are also called $\\textit{slack variables}.$\n",
    "\n",
    "The function therefore becomes:\n",
    "$$ \\min_{w, w_0,\\xi_1,...\\xi_n} \\frac{1}{2}\\|w\\|^2 + \\lambda\\sum_{i=1}^{n}\\xi_i$$\n",
    "subject to\n",
    "$$ y_i(x_i^Tw + w_0) \\geq 1 - \\xi_i \\ \\ for\\ i = 1,...,n$$\n",
    "$$ \\xi_i \\geq 0 \\ \\ for \\ \\ i = 1,...,n$$\n",
    "\n",
    "- If $\\lambda$ is very small, we're happy to misclassify.\n",
    "- For $\\lambda \\rightarrow \\infty$, we recover the original SVM because we want $\\xi_i=0$.\n",
    "- We can use cross-valudation to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#l2 penalty\n",
    "param_grid = {'tol': [1e-4, 1e-5, 1e-6],\n",
    "              'C': [0.1, 1, 10],\n",
    "              'max_iter': [500, 1000, 1500]}\n",
    "gs_lsvc = GridSearchCV(LinearSVC(), param_grid, cv=5)\n",
    "gs_lsvc.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_lsvc.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_lsvc.best_score_)\n",
    "print(\"Accuracy on Test Set:\", gs_lsvc.score(X_test, y_test))\n",
    "# LinearSVC does not return probability of prediction to calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "#l1 penalty\n",
    "lsvc = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "gs_lsvc_l1 = GridSearchCV(lsvc, {'C': [0.1, 1, 10]}, cv=5)\n",
    "gs_lsvc_l1.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_lsvc_l1.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_lsvc_l1.best_score_)\n",
    "print(\"Accuracy on Test Set:\", gs_lsvc_l1.score(X_test, y_test))\n",
    "# LinearSVC does not return probability of prediction to calculate AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Kernelizing SVM\n",
    "\n",
    "#### Kernel\n",
    "A kernel $K(\\cdot,\\cdot) : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a symmetric function defined as follows:\n",
    "\n",
    "For any set of $n$ data points $x_1,...,x_n \\in \\mathbb{R}^d$, the $n\\times n$ matrix $K$, where $K_{ij} = K(x_i, x_j)$, is $\\textit{positive semidefinite}$. (Note: The output of the kernel function is greater or equal to 0.)\n",
    "\n",
    "Intuitively, this means $K$ satisfies the properties of a covariance matrix.\n",
    "\n",
    "#### Mercer's theorem\n",
    "If the function $K(\\cdot,\\cdot)$ satisfies the above properties, then there exists a mapping $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^D$ such that\n",
    "$$k(x_i, x_j) = \\phi(x_i)^T\\phi(x_j).$$\n",
    "\n",
    "(Note: If this is satisfied for every single set of n vectors in $\\mathbb{R}^d$ where $n$ is arbitrary and the vectors themselves are arbitrary, **then there exists some function** $\\phi$, which is a function that takes in any particular $x$ and it performs the same function on that particular $x$ to map it to another space. And we can then have this dot product representation of the kernel function.)\n",
    "\n",
    "If we first define $\\phi(\\cdot)$, the mapping function, and then $K$, then this is obvious. However, sometimes we first define $K(\\cdot,\\cdot)$ and avoid ever using $\\phi(\\cdot)$.\n",
    "\n",
    "(Note: We use kernel to get results of the dot products of two $x_i$ and $x_j$ mapped to a higher place without actually mapping the $x_i$ and $x_j$ and calculating the dot product.)\n",
    "\n",
    "#### RBF\n",
    "By far the most popular kernel is the Gaussian kernel, also called the radiial basis function (RBF),\n",
    "$$ K(x, x') = \\alpha \\ exp\\big\\{ - \\frac{1}{b}\\|x-x'\\|^2 \\big\\}. $$\n",
    "\n",
    "- It takes into account proximity in $\\mathbb{R}^d$. Things close together in space have larger value (as defined by kernel width $b$).\n",
    "\n",
    "In this case, the mapping $\\phi(x)$ that produces the RBF kernel is $\\textit{infinite dimensional}$ (it's a continuous function instead of a vector). Therefore\n",
    "$$K(x,x') = \\int\\phi_t(x)\\phi_t(x')dt.$$\n",
    "\n",
    "(Note: there's a function of $x$ and $t$ such that this kernel results by integratign the product of those two functions.)\n",
    "- $K(x,x')$ is like a Gaussian on $x$ with $x'$ as the mean (or vice versa).\n",
    "\n",
    "#### Algorithm\n",
    "Map the data into higher dimension using the function $\\phi(x_i)$,\n",
    "$$ \\min_{w, w_0,\\xi_1,...\\xi_n} \\frac{1}{2}\\|w\\|^2 + \\lambda\\sum_{i=1}^{n}\\xi_i$$\n",
    "subject to\n",
    "$$ y_i(\\phi(x_i)^Tw + w_0) \\geq 1 - \\xi_i \\ \\ for\\ i = 1,...,n$$\n",
    "$$ \\xi_i \\geq 0 \\ \\ for \\ \\ i = 1,...,n$$\n",
    "\n",
    "To classify a new point:\n",
    "$$ y_0 = sign\\big(\\sum_{i=1}^{n}\\alpha_iy_i\\phi(x_0)T\\phi(x_i)+w_0)\\big)=\n",
    "sign(\\sum_{i=1}^{n}\\alpha_iy_iK(x_0,x_i)+w_0\\big)$$\n",
    "\n",
    "- We're still learning a linear classifier, in the higher dimensional map space, but when we look at what the decision is in the original space, we get non-linear decision boundaries.\n",
    "- In practice, we choose a kernel function (e.g., RBF) and use cross-validation for $\\lambda$ parameter and RBF kernel width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'gamma': [1e-2, 1/753, 1e-4],\n",
    "              'tol': [1e-3, 1e-4, 1e-5],\n",
    "              'max_iter': [500, 1000]}\n",
    "gs_svc = GridSearchCV(SVC(probability=True), param_grid, cv=5)\n",
    "gs_svc.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_svc.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_svc.best_score_)\n",
    "\n",
    "y_pred_prob = gs_svc.predict_proba(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_svc.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far support vector machine with kernel generates the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Decision Tree\n",
    "A decision tree maps input $x \\in \\mathbb{R}^d$ to output $y$ using binary decision rules:\n",
    "- Each node in the tree has a splitting rule.\n",
    "- Each leaf node is associated with an output value (outputs can repeat)\n",
    "\n",
    "Each splitting rule is of the form $h(x) = \\mathbb{I}\\{x_j>t\\}$ for some dimension $j$ of $x$ and $t\\in\\mathbb{R}$. Using these transition rules, a path to a leaf node gives the prediction.\n",
    "\n",
    "The basic method for learning tree is with a top-down greedy algorithm. Measure of quality of prediction include\n",
    "1. Classificataion error: $1 - \\max_kp_k$\n",
    "2. Gini index: $1 - \\sum_{k}p_k^2$\n",
    "3. Entrophy: $-\\sum_{k}p_k\\ln p_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [3, 5, 8, None],\n",
    "              'min_samples_split': [2, 3, 5]}\n",
    "gs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "gs_dt.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_dt.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_dt.best_score_)\n",
    "\n",
    "y_pred_prob = gs_dt.predict_prob(X_test)[:,1]\n",
    "print(\"Accuracy on Test Set:\", gs_dt.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model Improvement\n",
    "\n",
    "### Feature Engineering\n",
    "So far each observation of a patient has been treated as one instance. To extract more information for each patient, the three observations will be grouped into one and the mean, max, and min of each feature will be calculated, so that each instance represent as much information for one patient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.groupby('id').mean().drop(['class'], axis=1).add_suffix('_mean')\n",
    "df_min = df.groupby('id').min().drop(['class'], axis=1).add_suffix('_min')\n",
    "df_max = df.groupby('id').max().drop(['class'], axis=1).add_suffix('_max')\n",
    "X = pd.concat([df_mean, df_min, df_max], axis=1,\n",
    "              join_axes = [df.groupby('id').mean().index])\n",
    "X['gender'] = df[['gender', 'id']].drop_duplicates().reset_index(['id']).drop(['id'], axis=1)\n",
    "# Get X and y datasets\n",
    "#X = df_new.iloc[:,:-1]\n",
    "y = df[['class', 'id']].drop_duplicates().reset_index(['id']).drop(['id'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Now there are 2257 features in the dataset. Considering the massive number of features, using PCA to reduce features and leave only more discriminative ones would improve modeling speed and potentially prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pca.fit(scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame({'variance_ratio': pca.explained_variance_ratio_})\n",
    "df_pca['cumulated_ratio'] = df_pca['variance_ratio'].cumsum()\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_pca.index[:100], df_pca['variance_ratio'][:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_pca.index[:100], df_pca['cumulated_ratio'][:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.8, 0.85, 0.9, 0.95]:\n",
    "    print('Top {} features explains {} of the variance.'.format(df_pca[df_pca['cumulated_ratio'] > i].index[0], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline\n",
    "Use pipeline on standard scaler, PCA, and SVC, the best model so far, to see if there's any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([('Scaler', StandardScaler()),\n",
    "                     ('PCA', PCA()),\n",
    "                     ('SVC', SVC(probability=True))])\n",
    "parameters = {'PCA__n_components': [100, 150, 200],\n",
    "              'SVC__C': [0.1, 1, 10, 100],\n",
    "              'SVC__gamma': [1e-3, 1e-4, 1e-5],\n",
    "              'SVC__tol': [1e-3, 1e-4, 1e-5],\n",
    "              'SVC__max_iter': [500, 1000, 1500]}\n",
    "gs_pca_svc = GridSearchCV(pipeline, param_grid=parameters, cv=5)\n",
    "gs_pca_svc.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", gs_pca_svc.best_params_)\n",
    "print(\"Accuracy on Training Set:\", gs_pca_svc.best_score_)\n",
    "\n",
    "y_pred_prob = gs_pca_svc.predict_proba(X_test)\n",
    "print(\"Accuracy on Test Set:\", gs_pca_svc.score(X_test, y_test))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature engineering and PCA, the accuracy on test set and the AUC both improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply pipeline on all models to compare the best performance on accuracy and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.simplefilter('ignore')\n",
    "kNN = GridSearchCV(Pipeline([('Scaler', StandardScaler()), \n",
    "                             ('PCA', PCA()), ('kNN', KNeighborsClassifier())]),\n",
    "                   param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                               'kNN__n_neighbors': np.arange(3, 6)}, cv=5)\n",
    "\n",
    "NB = GridSearchCV(Pipeline([('scaler', StandardScaler()), ('PCA', PCA()), ('NB', GaussianNB())]),\n",
    "                  param_grid={'PCA__n_components': [100, 150, 200]}, cv=5)\n",
    "\n",
    "LR = GridSearchCV(Pipeline([('Scaler', StandardScaler()), \n",
    "                            ('PCA', PCA()), ('LR', LogisticRegression())]),\n",
    "                  param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                              'LR__penalty': ['l1', 'l2'],\n",
    "                              'LR__C': [0.1, 1, 10, 100],\n",
    "                              'LR__tol': [1e-4, 1e-5], \n",
    "                              'LR__max_iter': [100, 500]}, cv=5)\n",
    "\n",
    "SVM2 = GridSearchCV(Pipeline([('Scaler', StandardScaler()), \n",
    "                              ('PCA', PCA()), ('SVC', LinearSVC())]),\n",
    "                    param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                                'SVC__tol': [1e-4, 1e-5, 1e-6],\n",
    "                                'SVC__C': [0.1, 1, 10],\n",
    "                                'SVC__max_iter': [500, 1000, 1500]}, cv=5)\n",
    "\n",
    "SVM1 = GridSearchCV(Pipeline([('Scaler', StandardScaler()), ('PCA', PCA()), \n",
    "                              ('SVC', LinearSVC(loss='l2', penalty='l1', dual=False))]),\n",
    "                    param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                                'SVC__C': [0.1, 1, 10]}, cv=5)\n",
    "\n",
    "SVM = GridSearchCV(Pipeline([('Scaler', StandardScaler()), ('PCA', PCA()),\n",
    "                             ('SVC', SVC(probability=True))]),\n",
    "                   param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                               'SVC__C': [0.1, 1, 10],\n",
    "                               'SVC__gamma': [1e-2, 1/753, 1e-4,],\n",
    "                               'SVC__tol': [1e-3, 1e-4, 1e-5],\n",
    "                               'SVC__max_iter': [500, 1000]}, cv=5)\n",
    "\n",
    "DT = GridSearchCV(Pipeline([('Scaler', StandardScaler()), \n",
    "                            ('PCA', PCA()), ('DT', DecisionTreeClassifier())]),\n",
    "                  param_grid={'PCA__n_components': [100, 150, 200],\n",
    "                              'DT__criterion': ['gini', 'entropy'],\n",
    "                              'DT__max_depth': [3, 5, 8, None],\n",
    "                              'DT__min_samples_split': [2, 3, 5]}, cv=5)\n",
    "\n",
    "models = {'Naive Bayes': NB, 'Logistic Regression': LR, \n",
    "          'SVM-L2': SVM2, 'SVM-L1': SVM1, 'SVM': SVM, 'Decision Tree': DT}\n",
    "\n",
    "for k in models:\n",
    "    models[k].fit(X_train, y_train)\n",
    "    print(\"Best Parameters of {}:\".format(k), models[k].best_params_)\n",
    "    print(\"Accuracy on Training Set:\", models[k].best_score_)\n",
    "    print(\"Accuracy on Test Set:\", models[k].score(X_test, y_test))\n",
    "    if k not in ['SVM-L2', 'SVM-L1']:\n",
    "        print(\"AUC:\", roc_auc_score(y_test, models[k].predict_proba(X_test)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model with kernel trick got the best results, followed by logistic regression and linear SVC with L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "Here only the best models, SVC and logistic regression are focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_svm = SVM.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_lr = LR.predict_proba(X_test)[:,1]\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_pred_prob_svm)\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_prob_lr)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(fpr_svm, tpr_svm, label = 'Support Vector Machine')\n",
    "plt.plot(fpr_lr, tpr_lr, label = 'Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems logistic regression has bigger area under ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine:\")\n",
    "print(classification_report(y_test, SVM.predict(X_test)))\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine:\")\n",
    "print(classification_report(y_test, LR.predict(X_test)))\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: Concepts for Logistic Regression:\n",
    "\n",
    "#### A1. Binay Classification Type\n",
    "Input $x_i \\in \\mathbb{R}^b$ and output $y_i \\in {\\pm1}$\n",
    "\n",
    "we define a $\\textit{classifier f}$, which makes prediction $y_i = f(x_i, \\Theta)$ based on a function of $x_i$ and parameters $\\Theta$. In other works $f: \\mathbb{R}^d \\rightarrow {-1, +1}$\n",
    "\n",
    "In **Bayes classificaiton** framework, $\\Theta$ contains:\n",
    "1.  class prior probabilities on $y$,\n",
    "2. parameters for calss-dependent distribution on $x$.\n",
    "\n",
    "In **linear classification** framework, the prediction is linear in the parameters $\\Theta$.\n",
    "\n",
    "**Bayes classification** and **linear classification** are connected through ***log odds***.\n",
    "\n",
    "#### A2. Log Odds\n",
    "With Bayes classifier, we declare class $y=1$ if\n",
    "$$ p(x|y = 1)P(y = 1) > p(x|y = 0)P(y = 0) $$\n",
    "\n",
    "$$ \\Updownarrow $$\n",
    "\n",
    "$$ \\ln\\frac{p(x|y = 1)P(y = 1)}{p(x|y = 0)P(y = 0)} > 0 $$\n",
    "\n",
    "The second line is referred to as the $\\textit{log odds}$.\n",
    "\n",
    "#### A3. Lineaer Discriminant Analysis\n",
    "In the case where $p(x|y) = N(x| \\mu_y, \\Sigma)$ **(a single Gaussian with a shared covariance matrix)**\n",
    "\n",
    "$$ \\ln\\frac{p(x|y = 1)P(y = 1)}{p(x|y = 0)P(y = 0)} =\n",
    "\\ln\\frac{\\pi_1}{\\pi_0} - \\frac{1}{2}(\\mu_0 + \\mu_1)^T\\Sigma^{-1}(\\mu_1 - \\mu_0) + x^T\\Sigma^{-1}(\\mu_1 - \\mu_0)$$\n",
    "\n",
    "This is also called ***lineaer discriminant analysis*** (used to be called LDA).\n",
    "\n",
    "S0 we ca write the decision rule for the Bayes classifer as a linear one:\n",
    "$$ f(x) = sign(x^Tw + w_0) $$\n",
    "where\n",
    "\n",
    "$$ w_0 = \\ln\\frac{\\pi_1}{\\pi_0} - \\frac{1}{2}(\\mu_0 + \\mu_1)^T\\Sigma^{-1}(\\mu_1 - \\mu_0) $$\n",
    "$$ w = \\Sigma^{-1}(\\mu_1 - \\mu_0) $$\n",
    "\n",
    "This Bayes classifier is one instance of a linear classifier.\n",
    "\n",
    "Setting $w_0$ and $w$ this way may be too restrictive - it assumes single Gaussian with shared covariance. If we relax what values $w_0$ and $w$ can take we can do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B: Linear Classifiers\n",
    "\n",
    "#### B1. Definition\n",
    "A $\\textit{binary linear classifier}$ is a function of the form\n",
    "$$f(x) = sing(X^Tw + w_0),$$\n",
    "where $w \\in \\mathbb{R}^d$ and $w_0 \\in \\mathbb{R}$. Since the goal is to learn $w, w_0$ from the data, we are assuming that $\\textit{linear separability}$ in $x$ is an accurate property of the classes.\n",
    "\n",
    "#### B2. Linear Separability\n",
    "Two sets $A, B \\subset \\mathbb{R}^d$ arfe called linearly separable if\n",
    "\n",
    "$$x^Tw + w_0 > 0 \\ \\ if \\ \\ x \\in A (e.g, class +1)$$\n",
    "$$x^Tw + w_0 < 0 \\ \\ if \\ \\ x \\in B (e.g, class -1)$$\n",
    "\n",
    "The pair $(w, w_0)$ defines an $\\textit{affine hyperplane}$. It is important to develop the right geometric understanding about what this is doing.\n",
    "\n",
    "#### B3. Two methods:\n",
    "\n",
    "- **Least squares:** One simple idea is to treat classification as a regression problem. However, using regression for classification problem is not robust because it's sensitive to outliers\n",
    "- **Perceptron:** The perceptron represents a first attempt at linear classification by directly learning the hyper plane defined by $w$. It is not used as mush anymore because of some drawbacks: convergence issues and the assumption on linear seperability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References for Model Introduction and Algorithms\n",
    "- Applied Machine Learning Certification - Columnbia Engineering Executive Education\n",
    "- Post Graduate Diploma of Applied Machine Learning and Artificial Intelligence - Columnbia Engineering Executive Education\n",
    "\n",
    "#### Note: The coding was done through personal works and researches and not borrowed from the certification course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
