{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Parkinson's Disease\n",
    "This notebook uses the the dataset from [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification) to practice classification problem through popular classification models. For each model, the algorithm, and a related topic if any, will be discussed before application. Afterwards, there will be a review on modeling performance. \n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "### I. Data Preparation\n",
    "- Load and Process Data\n",
    "- Problem Definition\n",
    "\n",
    "### II. Modeling\n",
    "\n",
    "Each modeling section consists of **an brief intro, the algorithm, discussion on related topics, and application** on the dataset.\n",
    "\n",
    "1. kNN\n",
    "    - Non-parametric Models\n",
    "    - Algorithm\n",
    "2. Naive Bayes  \n",
    "    - Bayes Classifier\n",
    "    - Algorithm\n",
    "    - Generative Model vs. Discriminative Model\n",
    "3. Logistic Regression\n",
    "    - Sigmoid Function\n",
    "    - Maximum Likelihood Estimation\n",
    "    - Algorithm\n",
    "    - (Also see Appendix A & B for related topics)\n",
    "4. Support Vector Machine\n",
    "    - Convex Sets and Convex Hulls\n",
    "    - Algorithm\n",
    "    - Soft-Margin SVM\n",
    "\n",
    "5. Kernel SVM\n",
    "    - Kernel\n",
    "    - Mercer's theorem\n",
    "    - RBF\n",
    "    - Algorithm\n",
    "6. Decision Tree\n",
    "\n",
    "### III. Model Selection\n",
    "- Cross Validation\n",
    "- ROC Curve\n",
    "- Classification Report\n",
    "\n",
    "### Appendix\n",
    "- Appendix A: Concepts for Logistic Regression\n",
    "    - A1. Binary Classification\n",
    "    - A2. Log Odds\n",
    "    - A3. Linear Discriminant Analysis\n",
    "\n",
    "- Appendix B: Linear Classifiers\n",
    "    - B1. Definition\n",
    "    - B2. Linear Separability\n",
    "    - B3. Methods\n",
    "    \n",
    "### References for Model Introduction and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preparation\n",
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../project-parkinsons-disease-classification/data/pd_speech_features.csv', \n",
    "                 header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 755)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 755 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0   0       1  0.85247  0.71826  0.57227        240               239   \n",
       "1   0       1  0.76686  0.69481  0.53966        234               233   \n",
       "2   0       1  0.85083  0.67604  0.58982        232               231   \n",
       "3   1       0  0.41121  0.79672  0.59257        178               177   \n",
       "4   1       0  0.32790  0.79782  0.53028        236               235   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  ...    \\\n",
       "0          0.008064            0.000087       0.00218  ...     \n",
       "1          0.008258            0.000073       0.00195  ...     \n",
       "2          0.008340            0.000060       0.00176  ...     \n",
       "3          0.010858            0.000183       0.00419  ...     \n",
       "4          0.008162            0.002669       0.00535  ...     \n",
       "\n",
       "   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                     1.5620                     2.6445   \n",
       "1                     1.5589                     3.6107   \n",
       "2                     1.5643                     2.3308   \n",
       "3                     3.7805                     3.5664   \n",
       "4                     6.1727                     5.8416   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                     3.8686                     4.2105   \n",
       "1                    23.5155                    14.1962   \n",
       "2                     9.4959                    10.7458   \n",
       "3                     5.2558                    14.0403   \n",
       "4                     6.0805                     5.7621   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                     5.1221                     4.4625   \n",
       "1                    11.0261                     9.5082   \n",
       "2                    11.0177                     4.8066   \n",
       "3                     4.2235                     4.6857   \n",
       "4                     7.7817                    11.6891   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                     2.6202                     3.0004   \n",
       "1                     6.5245                     6.3431   \n",
       "2                     2.9199                     3.1495   \n",
       "3                     4.8460                     6.2650   \n",
       "4                     8.2103                     5.0559   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  class  \n",
       "0                    18.9405      1  \n",
       "1                    45.1780      1  \n",
       "2                     4.7666      1  \n",
       "3                     4.0603      1  \n",
       "4                     6.1164      1  \n",
       "\n",
       "[5 rows x 755 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column, class, is the target variable that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of attribute datatypes:\n",
      "float64    749\n",
      "int64        6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Summary of attribute datatypes:\\n', df.dtypes.value_counts(), sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 3 records for each patient, and all the columns, except for gender, seem to be numeric. Therefore, gender will be transformed into categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender = df.gender.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id column and get X and y datasets\n",
    "X = df.drop('id', axis = 1).iloc[:,:-1]\n",
    "y = df.drop('id', axis = 1).iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5466</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5530</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5399</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9761</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8832</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0      1  0.85247  0.71826  0.57227        240               239   \n",
       "1      1  0.76686  0.69481  0.53966        234               233   \n",
       "2      1  0.85083  0.67604  0.58982        232               231   \n",
       "3      0  0.41121  0.79672  0.59257        178               177   \n",
       "4      0  0.32790  0.79782  0.53028        236               235   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  \\\n",
       "0          0.008064            0.000087       0.00218      0.000018   \n",
       "1          0.008258            0.000073       0.00195      0.000016   \n",
       "2          0.008340            0.000060       0.00176      0.000015   \n",
       "3          0.010858            0.000183       0.00419      0.000046   \n",
       "4          0.008162            0.002669       0.00535      0.000044   \n",
       "\n",
       "             ...              tqwt_kurtosisValue_dec_27  \\\n",
       "0            ...                                 1.5466   \n",
       "1            ...                                 1.5530   \n",
       "2            ...                                 1.5399   \n",
       "3            ...                                 6.9761   \n",
       "4            ...                                 7.8832   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                     1.5620                     2.6445   \n",
       "1                     1.5589                     3.6107   \n",
       "2                     1.5643                     2.3308   \n",
       "3                     3.7805                     3.5664   \n",
       "4                     6.1727                     5.8416   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                     3.8686                     4.2105   \n",
       "1                    23.5155                    14.1962   \n",
       "2                     9.4959                    10.7458   \n",
       "3                     5.2558                    14.0403   \n",
       "4                     6.0805                     5.7621   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                     5.1221                     4.4625   \n",
       "1                    11.0261                     9.5082   \n",
       "2                    11.0177                     4.8066   \n",
       "3                     4.2235                     4.6857   \n",
       "4                     7.7817                    11.6891   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                     2.6202                     3.0004   \n",
       "1                     6.5245                     6.3431   \n",
       "2                     2.9199                     3.1495   \n",
       "3                     4.8460                     6.2650   \n",
       "4                     8.2103                     5.0559   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  \n",
       "0                    18.9405  \n",
       "1                    45.1780  \n",
       "2                     4.7666  \n",
       "3                     4.0603  \n",
       "4                     6.1164  \n",
       "\n",
       "[5 rows x 753 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    564\n",
       "0    192\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    338\n",
       "0    115\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "**Input:** $X \\in \\mathbb{R}^{d}, d = 243$\n",
    "\n",
    "**Output:** $Y = \\{0, 1\\}$\n",
    "\n",
    "**Classifier:** Calssification uses a function $f$ (called a classifier) to map input $x$ to class $y$. \n",
    "\n",
    "$y = f(x) : f$ takes in $x \\in X$ and declares its class to be $y \\in Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. k-Nearest Neighbors Classifier (kNN)\n",
    "KNNs classify the unseen instance based on the K points in the training set which are nearest to it. It is a **non-parametric method**. \n",
    "\n",
    "#### Non-parametric Models\n",
    "Non-parametric models differ from parametric models in that the model structure is not specified a priori but is instead determined from data. The term non-parametric is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Nonparametric_statistics#Non-parametric_models\n",
    "\n",
    "#### Algorithm\n",
    "Given data $(x_1, y_1),...,(x_n, y_n)$, construct the $k$-NN classifier as follows:\n",
    "For a new input $s$,\n",
    "1. Return the $k$ points closest to $x$, indexed as $x_{i_1},...x_{i_k}$.\n",
    "2. Return the majority-vote of $y_{i_1}, y_{i_2},..., y_{i_k}$.\n",
    "The default distance for data in $\\mathbb{R}^d$ is the Euclidean one:\n",
    "$$\\|u-v\\|_2 = \\big(\\sum_{i=1}^{d}(u_i-v_i)^2\\big)^\\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Neighbors:  {'n_neighbors': 7}\n",
      "Accuracy on Training Set:  0.757174392936\n",
      "Accuracy on Test Set:  0.722772277228\n",
      "AUC:  0.557292265257\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(3, 13)}\n",
    "gs_kNN = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "gs_kNN.fit(X_train, y_train)\n",
    "y_pred = gs_kNN.predict(X_test)\n",
    "print(\"Best Number of Neighbors: \", gs_kNN.best_params_)\n",
    "print(\"Accuracy on Training Set: \", gs_kNN.best_score_)\n",
    "print(\"Accuracy on Test Set: \", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe how the accuracy change as k grows\n",
    "Note: The accuracy is different from the results from grid search because the whole training set is used to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training and test errors by k\n",
    "training_error = list()\n",
    "test_error = list()\n",
    "for k in np.arange(3, 13):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    training_error.append(knn.score(X_train, y_train))\n",
    "    test_error.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1088f3d0390>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_error, label = 'training error')\n",
    "plt.plot(test_error, label = 'test error')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, 10), np.arange(3, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grid search and cross validation, **k=5** generates the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems kNN model with k=11 achieves better but similar performance with k=5 in terms of accuracy and AUC. However, the results are not very impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes \n",
    "\n",
    "With Naive Bayes classifier we predict the class of a new $x$ to be the most probable lable given model and training data $(x_1, y_1), ..., (x_n, y_n)$.\n",
    "\n",
    "#### Bayes Classifier\n",
    "Before talking about the algorithm of Naive Bayes, we need to know **Bayes Classifer**:\n",
    "\n",
    "$$f(x) = \\operatorname*{arg\\,max}_{y \\in Y} P(Y = y| X = x)$$\n",
    "\n",
    "For a particular input $x$, predict the label to be the most probable label conditioned on $x$ according to the true underlying distribution given to us from nature.\n",
    "\n",
    "From Bayes rule we equivalently have\n",
    "$$ f(x) \\approx \\operatorname*{arg\\,max}_{y \\in Y} P(Y = y) \\times P(Y = y| X = x) $$\n",
    "- $P(Y = y)$ is called the $\\textit{class prior}$.\n",
    "- $P(X = x|Y = y)$ is called the $\\textit{class conditional distribution}$ of X.\n",
    "- In practice we don't know either of these, so we approximate them.\n",
    "\n",
    "Aside: If $X$ is a continuous-valued random variable, replace $P(X = x|Y = y)$ with class conditional density $p(x|Y=y)$.\n",
    "\n",
    "Problem: We can't construct the Bayes classifier without knowing $P(Y = y|X = x)$, or equv., $P(X = x|Y = y)$ and $P(Y = y)$. All we have are labeled examples drowm from the distribution.\n",
    "\n",
    "#### Naive Bayes Algorithm\n",
    "We have to $\\textit{define } p(X = x|Y = y)$.\n",
    "\n",
    "Naive Bayes is a Bayes classifier that makes the assumption\n",
    "$$p(X = x|Y = y) = \\prod_{j=1}^{d}p_j(x(j)|Y = y),$$\n",
    "i.e., it treats the dimension of $X$ as $\\textit{conditionally independent}$ given $y$. \n",
    "\n",
    "Note: Each mimension might not be independent with each other, but they are conditionally independent given y.\n",
    "\n",
    "#### Discriminative Model vs. Generative Model\n",
    "\n",
    "Bayes Classifer and Naive Bayes are ***generative models***. Unlike distriminative models, generative models consider the joint probability distribution on $X \\times y, P(X, y)$, make the prediction on the probability of each lable $y$ given $X$, $p(y|X = x)$, and then pick the most likely lable $y$.\n",
    "\n",
    "##### Discriminative Algorithms\n",
    "- Idea: model $p(y|x)$, conditional distribution of $y$ given $x$.\n",
    "- In Discriminative Algorithms: find a decision boundary that separates positive from negative example.\n",
    "- To predict a new example, chec on which side of the decision boundary it falls.\n",
    "- Model $p(y|x)$ directly\n",
    "\n",
    "##### Generative Algorithms\n",
    "- Idea: Build a model for what positive examples look like and build a different model for what negative example look like.\n",
    "- To predict a new example, match it with each of the models and see which match is best.\n",
    "- Model $p(x|y)$ and $p(y)$!\n",
    "- Use Bayes rule to obtain $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$\n",
    "\n",
    "The definition of generative and distriminative models:\n",
    "- a generative model is a model of the conditional probability of the observable $X$, given a target $y$, symbolically, $P(X|Y=y)$\n",
    "- a discriminative model is a model of the conditional probability of the target $Y$, given an observation $x$, symbolically, $P(Y|X=x)$\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Generative_model#Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.765676567657\n",
      "AUC:  0.684519020802\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy on test set: \", gnb.score(X_test, y_test))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression\n",
    "Let $(x_1, y_1),...,(x_n, y_n)$ be a set of binary labeled data with $y \\in {-1, +1}$. $\\textit{Logistic regression}$ models each $y_i$ as independently generated, with\n",
    "\n",
    "$$P(y_i = +1|x_i, w) = \\sigma(x_i^Tw), \\ \\ \\sigma(x_i; w) = \\frac{e^{x_iTw}}{1 + e^{x_iTw}}.$$\n",
    "\n",
    "#### Sigmoid Function\n",
    "From **Linear Discriminative Analysis**, we can directly plug in the **hyperplane representation** for the **log odds**. ***(See Appendix A: Concepts for Logistic Regression & Appendix B: Linear Classifiers)***\n",
    "\n",
    "$$\\ln\\frac{p(y = +1|x)}{p(y = -1|x)} = x^Tw + w_0$$\n",
    "\n",
    "Note: No restrictions on $w$ and $w_0$ compared to LDA.\n",
    "\n",
    "Setting $p(y = -1|x) = 1 - p(y = -1|x)$, solve for $p(y = +1|x)$ to find \n",
    "$$ p(y = +1|x) = \\frac{exp^{x^Tw + w_0}}{1 + exp^{x^Tw + w_0}} = \\sigma(x^tw + w_0). $$\n",
    "\n",
    "- This is called the sigmoid function\n",
    "- We have chosen $x^Tw + w_0$ as the $\\textit{link function}$ for log odds.\n",
    "- If $x^Tw > 0$, then $\\sigma(x^Tw) > 1/2$ and predict $y = +1$, and vice versa.\n",
    "- We now get **a confidence in our prediction** via the probability of $\\sigma (x^Tw)$.\n",
    "\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "Define $\\sigma_i(w) = \\sigma(x_i^Tw)$. The joint likihood of $y_1,...y_n$ is\n",
    "$$p(y_1,...,y_n|x_1,...,x_n, w) = \\prod_{i=1}^{n}p(y_i|x_i, w)\n",
    "= \\prod_{i=1}^{n}\\sigma_i(w)^{\\mathbb{I}(y_i=+1)}(1-\\sigma_i(w))^{\\mathbb{I}(y_i=-1)} \n",
    "= \\prod_{i=1}^{n}\\sigma_i(y_i \\cdot w)$$\n",
    "\n",
    "Note: here y = {+1, -1}\n",
    "\n",
    "we want to maximize this over $w$.\n",
    "\n",
    "The maximum likelihood solution for $w$ can be written \n",
    "$$ w_{ML} = \\operatorname*{arg\\,max}_{w} \\sum_{i=1}^{n}\\ln\\sigma_i(y_i\\cdot w)\n",
    "= \\operatorname*{arg\\,max}_{w} L $$\n",
    "\n",
    "We can't directly set $\\nabla_w L = 0$, so we need an iterative algorithm. At step $t$, we can update\n",
    "$$ w^{(t+1)}=w^{(t)}+\\eta\\nabla_wL \\ \\ \\nabla_wL = \\sum_{i=1}{n}(1-\\sigma_i(y_i\\cdot w))y_ix_i $$\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "**Input**: Training data $(x_1, y_1),...,(x_n, y_n)$ and step size $\\eta > 0$\n",
    "1. **Set** $w^{(1)} = \\overrightarrow{0}$\n",
    "2. **For step** $t = 1,2,...$ **do**\n",
    "    - Update $w^{(t+1)} = w^{(t)} + \\eta\\sum_{i=1}^{n}(1-\\sigma_i(y_i\\cdot w))y_ix_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Neighbors:  {'C': 1, 'max_iter': 100}\n",
      "Accuracy on Training Set:  0.766004415011\n",
      "Accuracy on Test Set:  0.762376237624\n",
      "AUC:  0.592403172049\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build logistic model with L2 regularization\n",
    "param_grid = {'C': [0.1, 1, 10], 'max_iter': [100, 250, 500]}\n",
    "gs_lr = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "y_pred = gs_lr.predict(X_test)\n",
    "print(\"Best Number of Neighbors: \", gs_lr.best_params_)\n",
    "print(\"Accuracy on Training Set: \", gs_lr.best_score_)\n",
    "print(\"Accuracy on Test Set: \", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25178024  0.74821976]\n",
      " [ 0.1974452   0.8025548 ]\n",
      " [ 0.37619111  0.62380889]\n",
      " [ 0.20545793  0.79454207]\n",
      " [ 0.3189436   0.6810564 ]\n",
      " [ 0.14211316  0.85788684]\n",
      " [ 0.39319857  0.60680143]\n",
      " [ 0.41941668  0.58058332]\n",
      " [ 0.15785083  0.84214917]\n",
      " [ 0.30737367  0.69262633]]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "228    1\n",
      "119    1\n",
      "257    1\n",
      "63     1\n",
      "437    1\n",
      "265    1\n",
      "484    1\n",
      "395    0\n",
      "416    1\n",
      "439    1\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = gs_lr.predict_proba(X_test)\n",
    "print(y_pred_prob[:10])\n",
    "print(y_pred[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the threshold to 60% to observe the accuracy and AUC\n",
    "y_pred_rev = [1 if i[1] > 0.6 else 0 for i in y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76897689768976896"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_rev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Logistic Regression\n",
    "# Logistic with penalty\n",
    "# Obsearch probability and change threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Support Vector Machine\n",
    "\n",
    "With two linearly separable classes, choose a hyperplane such that its distance to the **closest point in each class** is maximized to achieve good generalization (low prediction error).\n",
    "\n",
    "#### Convex Sets and Convex Hulls\n",
    "Where a seperating hyperplane may be placed depends on the \"outer\" points on the sets. Points in the center do not matter. In geometric terms, we can represent each class by the smallest convex set which contains all point in the class. This is called a $\\textit{convex hull}$.\n",
    "\n",
    "A convex hull is defined by all possible weighted averages of points in a set. That is, let $x_1,...x_n$ be the data coordinates. Every point $x_0$ in the convex hull can be reached by setting\n",
    "$$ x_0 = \\sum_{i=1}^{n}\\alpha_1x_1, \\ \\ \\alpha_i \\geq0, \\ \\ \\sum_{i=1}^{n}\\alpha_1 = 1,$$\n",
    "for some $(\\alpha_1,...\\alpha_n)$. No point outside the convex hull can be reached this way.\n",
    "\n",
    "#### Algorithm\n",
    "For $n$ seperate points $(x_1,y_1),...,(x_n,y_n)$ with $y_i \\in {\\pm1}$, solve:\n",
    "$$ \\min_{w, w_0} \\frac{1}{2}\\|w\\|^2 $$\n",
    "subject to\n",
    "$$ y_i(x_i^Tw + w_0) \\geq 1 \\ \\ for\\ i = 1,...,n$$\n",
    "\n",
    "- If there exists a hyperplan $H$ that separates the classes, we can scale $w$ so that $y_i(x_i^Tw+w_0)>1$ for all $i$.\n",
    "- This formula only has a solution when the classes are linearly separable.\n",
    "\n",
    "Solving above foluma would require $\\textit{Lagrange multipliers}$. After derived with $\\textit{Lagrange multipliers}$, the formula will eventually turn into:\n",
    "$$\\min_{\\alpha_1,...\\alpha_n}\\bigg|\\bigg(\\sum_{i\\in S_1}\\frac{\\alpha_i}{C}x_i\\bigg) - \n",
    "\\bigg(\\sum_{j\\in S_0}\\frac{\\alpha_j}{C}x_j\\bigg)\\bigg|^2,$$\n",
    "where \n",
    "- $S_i$ and $S_0$ are the sets of $x$ in class $+1$ and $-1$\n",
    "- $C:=\\sum_{i\\in S_1}\\alpha_u = \\sum_{j\\in S_0}\\alpha_j,\\ \\ \\alpha_i\\geq0$\n",
    "\n",
    "Therefore, the algorithm is to find the closest points in the convex hulls constructed from the data in class $+1$ and $-1$.\n",
    "\n",
    "#### Soft-Margin SVM\n",
    "\n",
    "If the data isn't linearly separable, permit training data be on wrong side of hyperplane at a cost by replacing the training rule $y_i(x_i^Tw + w_0) \\geq 1$  with \n",
    "$$y_i(x_i^Tw+w_0)\\geq 1 - \\xi_i, \\ \\ with \\ \\ \\xi_i \\geq0.$$\n",
    "\n",
    "The $\\xi_i$ are also called $\\textit{slack variables}.$\n",
    "\n",
    "The function therefore becomes:\n",
    "$$ \\min_{w, w_0,\\xi_1,...\\xi_n} \\frac{1}{2}\\|w\\|^2 + \\lambda\\sum_{i=1}^{n}\\xi_i$$\n",
    "subject to\n",
    "$$ y_i(x_i^Tw + w_0) \\geq 1 - \\xi_i \\ \\ for\\ i = 1,...,n$$\n",
    "$$ \\xi_i \\geq 0 \\ \\ for \\ \\ i = 1,...,n$$\n",
    "\n",
    "- If $\\lambda$ is very small, we're happy to misclassify.\n",
    "- For $\\lambda \\rightarrow \\infty$, we recover the original SVM because we want $\\xi_i=0$.\n",
    "- We can use cross-valudation to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Kernelizing SVM\n",
    "\n",
    "#### Kernel\n",
    "A kernel $K(\\cdot,\\cdot) : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a symmetric function defined as follows:\n",
    "\n",
    "For any set of $n$ data points $x_1,...,x_n \\in \\mathbb{R}^d$, the $n\\times n$ matrix $K$, where $K_{ij} = K(x_i, x_j)$, is $\\textit{positive semidefinite}$. (Note: The output of the kernel function is greater or equal to 0.)\n",
    "\n",
    "Intuitively, this means $K$ satisfies the properties of a covariance matrix.\n",
    "\n",
    "#### Mercer's theorem\n",
    "If the function $K(\\cdot,\\cdot)$ satisfies the above properties, then there exists a mapping $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^D$ such that\n",
    "$$k(x_i, x_j) = \\phi(x_i)^T\\phi(x_j).$$\n",
    "\n",
    "(Note: If this is satisfied for every single set of n vectors in $\\mathbb{R}^d$ where $n$ is arbitrary and the vectors themselves are arbitrary, **then there exists some function** $\\phi$, which is a function that takes in any particular $x$ and it performs the same function on that particular $x$ to map it to another space. And we can then have this dot product representation of the kernel function.)\n",
    "\n",
    "If we first define $\\phi(\\cdot)$, the mapping function, and then $K$, then this is obvious. However, sometimes we first define $K(\\cdot,\\cdot)$ and avoid ever using $\\phi(\\cdot)$.\n",
    "\n",
    "(Note: We use kernel to get results of the dot products of two $x_i$ and $x_j$ mapped to a higher place without actually mapping the $x_i$ and $x_j$ and calculating the dot product.)\n",
    "\n",
    "#### RBF\n",
    "By far the most popular kernel is the Gaussian kernel, also called the radiial basis function (RBF),\n",
    "$$ K(x, x') = \\alpha \\ exp\\big\\{ - \\frac{1}{b}\\|x-x'\\|^2 \\big\\}. $$\n",
    "\n",
    "- It takes into account proximity in $\\mathbb{R}^d$. Things close together in space have larger value (as defined by kernel width $b$).\n",
    "\n",
    "In this case, the mapping $\\phi(x)$ that produces the RBF kernel is $\\textit{infinite dimensional}$ (it's a continuous function instead of a vector). Therefore\n",
    "$$K(x,x') = \\int\\phi_t(x)\\phi_t(x')dt.$$\n",
    "\n",
    "(Note: there's a function of $x$ and $t$ such that this kernel results by integratign the product of those two functions.)\n",
    "- $K(x,x')$ is like a Gaussian on $x$ with $x'$ as the mean (or vice versa).\n",
    "\n",
    "#### Algorithm\n",
    "Map the data into higher dimension using the function $\\phi(x_i)$,\n",
    "$$ \\min_{w, w_0,\\xi_1,...\\xi_n} \\frac{1}{2}\\|w\\|^2 + \\lambda\\sum_{i=1}^{n}\\xi_i$$\n",
    "subject to\n",
    "$$ y_i(\\phi(x_i)^Tw + w_0) \\geq 1 - \\xi_i \\ \\ for\\ i = 1,...,n$$\n",
    "$$ \\xi_i \\geq 0 \\ \\ for \\ \\ i = 1,...,n$$\n",
    "\n",
    "To classify a new point:\n",
    "$$ y_0 = sign\\big(\\sum_{i=1}^{n}\\alpha_iy_i\\phi(x_0)T\\phi(x_i)+w_0)\\big)=\n",
    "sign(\\sum_{i=1}^{n}\\alpha_iy_iK(x_0,x_i)+w_0\\big)$$\n",
    "\n",
    "- We're still learning a linear classifier, in the higher dimensional map space, but when we look at what the decision is in the original space, we get non-linear decision boundaries.\n",
    "- In practice, we choose a kernel function (e.g., RBF) and use cross-validation for $\\lambda$ parameter and RBF kernel width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for SVM with kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Decision Tree\n",
    "A decision tree maps input $x \\in \\mathbb{R}^d$ to output $y$ using binary decision rules:\n",
    "- Each node in the tree has a splitting rule.\n",
    "- Each leaf node is associated with an output value (outputs can repeat)\n",
    "\n",
    "Each splitting rule is of the form $h(x) = \\mathbb{I}\\{x_j>t\\}$ for some dimension $j$ of $x$ and $t\\in\\mathbb{R}$. Using these transition rules, a path to a leaf node gives the prediction.\n",
    "\n",
    "The basic method for learning tree is with a top-down greedy algorithm. Measure of quality of prediction include\n",
    "1. Classificataion error: $1 - \\max_kp_k$\n",
    "2. Gini index: $1 - \\sum_{k}p_k^2$\n",
    "3. Entrophy: $-\\sum_{k}p_k\\ln p_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model Selection\n",
    "### Cross Validation\n",
    "Apply cross validation on best models to observe the variance of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: Concepts for Logistic Regression:\n",
    "\n",
    "#### A1. Binay Classification Type\n",
    "Input $x_i \\in \\mathbb{R}^b$ and output $y_i \\in {\\pm1}$\n",
    "\n",
    "we define a $\\textit{classifier f}$, which makes prediction $y_i = f(x_i, \\Theta)$ based on a function of $x_i$ and parameters $\\Theta$. In other works $f: \\mathbb{R}^d \\rightarrow {-1, +1}$\n",
    "\n",
    "In **Bayes classificaiton** framework, $\\Theta$ contains:\n",
    "1.  class prior probabilities on $y$,\n",
    "2. parameters for calss-dependent distribution on $x$.\n",
    "\n",
    "In **linear classification** framework, the prediction is linear in the parameters $\\Theta$.\n",
    "\n",
    "**Bayes classification** and **linear classification** are connected through ***log odds***.\n",
    "\n",
    "#### A2. Log Odds\n",
    "With Bayes classifier, we declare class $y=1$ if\n",
    "$$ p(x|y = 1)P(y = 1) > p(x|y = 0)P(y = 0) $$\n",
    "\n",
    "$$ \\Updownarrow $$\n",
    "\n",
    "$$ \\ln\\frac{p(x|y = 1)P(y = 1)}{p(x|y = 0)P(y = 0)} > 0 $$\n",
    "\n",
    "The second line is referred to as the $\\textit{log odds}$.\n",
    "\n",
    "#### A3. Lineaer Discriminant Analysis\n",
    "In the case where $p(x|y) = N(x| \\mu_y, \\Sigma)$ **(a single Gaussian with a shared covariance matrix)**\n",
    "\n",
    "$$ \\ln\\frac{p(x|y = 1)P(y = 1)}{p(x|y = 0)P(y = 0)} =\n",
    "\\ln\\frac{\\pi_1}{\\pi_0} - \\frac{1}{2}(\\mu_0 + \\mu_1)^T\\Sigma^{-1}(\\mu_1 - \\mu_0) + x^T\\Sigma^{-1}(\\mu_1 - \\mu_0)$$\n",
    "\n",
    "This is also called ***lineaer discriminant analysis*** (used to be called LDA).\n",
    "\n",
    "S0 we ca write the decision rule for the Bayes classifer as a linear one:\n",
    "$$ f(x) = sign(x^Tw + w_0) $$\n",
    "where\n",
    "\n",
    "$$ w_0 = \\ln\\frac{\\pi_1}{\\pi_0} - \\frac{1}{2}(\\mu_0 + \\mu_1)^T\\Sigma^{-1}(\\mu_1 - \\mu_0) $$\n",
    "$$ w = \\Sigma^{-1}(\\mu_1 - \\mu_0) $$\n",
    "\n",
    "This Bayes classifier is one instance of a linear classifier.\n",
    "\n",
    "Setting $w_0$ and $w$ this way may be too restrictive - it assumes single Gaussian with shared covariance. If we relax what values $w_0$ and $w$ can take we can do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B: Linear Classifiers\n",
    "\n",
    "#### B1. Definition\n",
    "A $\\textit{binary linear classifier}$ is a function of the form\n",
    "$$f(x) = sing(X^Tw + w_0),$$\n",
    "where $w \\in \\mathbb{R}^d$ and $w_0 \\in \\mathbb{R}$. Since the goal is to learn $w, w_0$ from the data, we are assuming that $\\textit{linear separability}$ in $x$ is an accurate property of the classes.\n",
    "\n",
    "#### B2. Linear Separability\n",
    "Two sets $A, B \\subset \\mathbb{R}^d$ arfe called linearly separable if\n",
    "\n",
    "$$x^Tw + w_0 > 0 \\ \\ if \\ \\ x \\in A (e.g, class +1)$$\n",
    "$$x^Tw + w_0 < 0 \\ \\ if \\ \\ x \\in B (e.g, class -1)$$\n",
    "\n",
    "The pair $(w, w_0)$ defines an $\\textit{affine hyperplane}$. It is important to develop the right geometric understanding about what this is doing.\n",
    "\n",
    "#### B3. Two methods:\n",
    "\n",
    "- **Least squares:** One simple idea is to treat classification as a regression problem. However, using regression for classification problem is not robust because it's sensitive to outliers\n",
    "- **Perceptron:** The perceptron represents a first attempt at linear classification by directly learning the hyper plane defined by $w$. It is not used as mush anymore because of some drawbacks: convergence issues and the assumption on linear seperability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References for Model Introduction and Algorithms\n",
    "- Applied Machine Learning Certification - Columnbia Engineering Executive Education\n",
    "- Post Graduate Diploma of Applied Machine Learning and Artificial Intelligence - Columnbia Engineering Executive Education\n",
    "\n",
    "#### Note: The coding was done through personal works and researches and not borrowed from the certification course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
